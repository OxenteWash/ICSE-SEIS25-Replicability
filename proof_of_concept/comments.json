{
    "F1": [
        "Thanks!",
        "Thank you i do when i install \"dynamo\" as a \"NO\" and its solved ",
        "> Try Dreambooth then, using captions and without reg images. If it still doesn't work, I'm sorry, but to create an SDXL ckpt without taking 15,000 hours, you'll need more VRAM.\n\nThank you, I will try!",
        "[primeinc](https://github.com/primeinc)'s solution works. Thanks. \n\nP.S : \"**validate_requirements.py**\" is not in _tools_ but in the _setup_ folder. \n",
        "> For batching multiple training commands into a script file, it would be nice to not have to individually save every tmpfiledbooth.toml file, perhaps also save as a file with name based on timestamp.\n> \n> In previous versions the print training command would have all the necessary arguments to run with accelerate. Now the multi-line command from \"print training command\" requires carets^ per line in windows batch file and needing to separately save each toml file with a new filename and editing the filename in the command, which can easily be prone to errors when trying to batch multiple commands.\n\n@keclee I am interested in batching multiple training commands. What is your process now with calling in a batch file? I dont quite understand the process. \n\nI used to copy and paste accelerate cli commands straight into console.  thank you",
        "Thanks for the update.",
        "Thanks @synystersocks that worked perfectly. It might be nice to have this as a 'default' SDXL preset. Currently it's unclear what preset to use and I think the one I was using last time was too heavy (or unoptimized) for my machine.\n\n```\n139/2640 [04:55<1:28:28,  2.12s/it, loss=0.119]\n```\n\nOn a RTX 4070.",
        "OK, thank you for the advise, i will do more test and submit again, i wonder if anyone has ever tested it with multi-GPU machine. now it runs , but seems still has some issues ",
        "I am using local gpu. btw I got clear by stopping processing on wsl, but on window cmd :) Thx!",
        "Thank you for your answer. I will also try to find a way to solve this problem again.",
        "Thanks, I missed that in the latest update. I will publish it in the next release. Merged in the dev2 branch if you want to use it.",
        "Thank you @Symbiomatrix for the PR but this part of code is maintained by kohya on his sd-scripts repo. Please submit the PR to so so he can merge in his repo. Once he merge it in I will be able to pull it in my repo for sue in the GUI.",
        "Hello @zoezhu,\n\nThank you for the PR but I don't maintain that part of the code. I only wrap kohya's code in the GUI.\n\nI suggest you open a PR directly with kohya_ss in his sd-scripts repo so he can merge this fix.\n\nCheers,\n\nBernard",
        "I am using a local Linux GPU server from my company, which only allows command line and LAN access. Currently, I only need the lora training functionality from this repository. Thank you for merging it.",
        "No i haven't. I'll definitely try this tomorrow though. Thank you",
        "Hi, thank you for taking the time to make this PR to fix the bug. I have resolved the issue more globally by resolving the issue when no logging_dir is provided. As a result I will not be merging the PR... but thank you for bringing it to my attention.",
        "Thakf for the fix.",
        "thank you for such amazing content ,i just wanted to ask that i havr given a training image dataset of 5 images and it shows me 4 hrs .but i wanted to run in p100 gpu provided by kaggke\n\nso i made changes like\n!accelerate launch --num_gpus=1 \"./sdxl_train_network.py\" \\\n --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-xl-base-1.0\" \\\n --train_data_dir=\"/kaggle/working/training-data/img\" \\\n --reg_data_dir=\"/kaggle/working/training-data/reg\" \\\n --output_dir=\"/kaggle/working/my-dreambooth-lora\" \\\n --logging_dir=\"/kaggle/working/training-data/log\" \\\n --resolution=\"1024,1024\" \\\n --network_alpha=\"1\" \\\n --network_dim=32 \\\n --save_model_as=safetensors \\\n --network_module=networks.lora \\\n --text_encoder_lr=0.0004 \\\n --unet_lr=0.0004 \\\n --output_name=\"sdxl-dreambooth-lora\" \\\n --lr_scheduler_num_cycles=\"8\" \\\n --no_half_vae \\\n --learning_rate=\"0.0004\" \\\n --lr_scheduler=\"constant\" \\\n --train_batch_size=\"1\" \\\n --max_train_steps=$(($(ls /kaggle/input/training-images | wc -l) * 400)) \\\n --save_every_n_epochs=\"1\" \\\n --mixed_precision=\"amp\" \\\n --save_precision=\"amp\" \\\n --optimizer_type=\"Adafactor\" \\\n --optimizer_args scale_parameter=False relative_step=False warmup_init=False \\\n --max_data_loader_n_workers=\"0\" \\\n --gradient_checkpointing \\\n --xformers \\\n --bucket_no_upscale \\\n --noise_offset=0.0 \\\n --lowram \\\n --mem_eff_attn\n \n still itd not wokring can you please help me over here",
        "Can someone elaborate on how multi GPU works? If I select this when installing, do I need to do anything else, or it's all under the hood?\nHave 2x 4090 and would be great to speed up training by running both. Can't find a page in docs here about how this actually functions or what effect this selection has when installing. \nAlso, is it possible to run for now with single (following installation tutorials and kind of just want to do what they do), and then later, change to multi-GPU?\nThanks",
        "Thank you for the PR. Nice idea!",
        "wow indeed to enable training of the text encoders you need to add '--train_text_encoder' in the 'Additional Parameters' field under the Advanced tab. Thanks for pointing this out, this whole time I thought I was training those and wasn't :\\",
        "thanks, might have helped, but what I also needed was:\n```bash\napt-get install python3.10-tk\napt install python3.10-distutils\ncurl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\npython3.10 get-pip.py\n```\nIf you do not specify 3.10 explicitly you get python3.8 which is ignored when running gui.sh\nnow I have it running on RunPod Fast Stable Diffusion. Not however, that this installation seems to interfere with the stable-diffusion-webui, at least I get an error now, running step 2: `Install/Update AUTOMATIC1111 repo` easiest is to reset the pod to install that.\n",
        "Awesome! Many thanks!",
        "@bmaltais alright, thanks!",
        "> I'm getting\n> \n> ```\n> cp: cannot stat '/workspace/kohya_ss/config_files/accelerate/default_config.yaml': No such file or directory\n> ```\n> \n> also\n> \n> ```\n> root@82ae62b9b7b9:/workspace/stable-diffusion-webui/kohya_ss# ./setup.sh -p\n> ./setup.sh: illegal option -- p\n> ```\n> \n> `--public` works though (and yes, the instance can be accessed!)\n\nIf you run the script with `-d /workspace/stable-diffusion-webui/kohya_ss` does that config error go away? I think it may be because you are using the default install location which is `/workspace/kohya_ss` instead of your real one. I'm pretty sure I got the -p issue as well. \n\nAnd no problem on the wait; I just appreciate you heading off hundreds of user issues \ud83d\ude04 "
    ],
    "F2": [
        "The last time I upgraded the bitsandbytes version to a higher release  than 0.35.0 I received a flury of issues with results of training being bad compared to 0.35.0... so I am hesitant to merge... Have you checked if the training results are comparable with 0.35.0?",
        "Thanks for the PR but this is a file maintained by kohya in his sd-scripts repo. I merely sync his code in my repo to wrap it in a GUI. I suggest to open the PR directly on his repo so he can integrate it upstream and I will then receive it when I sync his code in my repo ",
        "Good morning @ebabchick \n\nMany thanks for the PR. Unfortunatly it should be done against kohya_ss sd-scripts repo as I only mirror his code to wrap it in a GUI. So even if I could update the code in my repo with your PR it woud get overwritten the next time @kohya-ss merge code update in his own repo.\n\nHere is the direct link to his repo so you can re-create the PR there.: https://github.com/kohya-ss/sd-scripts\n\nRegards,\n\nBernard",
        "I second that request, I had a similar question here : #370\n\nAlthough, I don't think that Bernard can do much about it. It would be more for kohya_ss actually ?",
        "One of my big worry right now is that I am not sure if I will be able to fix issue with the installer if you don't provide ongoing support for it. The solution look great but it appear to be very complex and with complexity comes difficult support.\n\nHopefully once everything is figured out it will not be prone to failure as I update the code base. It should not... but it is worrying me a bit.",
        "Hello @tomguluson92 ,\n\nI don't maintain the file you modified in the PR. It is maintained by kohya_ss in his repo. I only pull his code in mine and wrap it in a GUI.\n\nI suggest you create a PR directly with his repo for this: https://github.com/kohya-ss/sd-scripts",
        "Indeed. I did not create the template. Hope the creator can adapt it to the latest release.",
        "> For batching multiple training commands into a script file, it would be nice to not have to individually save every tmpfiledbooth.toml file, perhaps also save as a file with name based on timestamp.\n> \n> In previous versions the print training command would have all the necessary arguments to run with accelerate. Now the multi-line command from \"print training command\" requires carets^ per line in windows batch file and needing to separately save each toml file with a new filename and editing the filename in the command, which can easily be prone to errors when trying to batch multiple commands.\n\n@keclee I am interested in batching multiple training commands. What is your process now with calling in a batch file? I dont quite understand the process. \n\nI used to copy and paste accelerate cli commands straight into console.  thank you",
        "Hello @hopl1t \n\nthis look like a great pr but it is modifying kohya sd-scripts files I clone to my repo. May I suggest you create a copy of this PR directly on his repo? When he merge the change I will then get the update when I pull his code into my repo.",
        "> I don't want to overclutter the UI. For that there is always the cki to open a custom log folder\n\nI thought you could put it between those large \"Start tensorboard\" \"Stop tensorboard\" buttons. A \"Select log\" with a button to start. :)",
        "Hi, thank you for taking the time to make this PR to fix the bug. I have resolved the issue more globally by resolving the issue when no logging_dir is provided. As a result I will not be merging the PR... but thank you for bringing it to my attention.",
        "Good question. I suggest you reach out to the author directly on his repo: https://github.com/KohakuBlueleaf/LyCORIS",
        "Adding @jstayco as this will probably conflict with the new launcher.py solution... I will let him assess the impact.",
        "Not sure if I understand what you mean, but I would argue it is not possible and I don't see a benefit. And what would be the purpose any ways? Fine tuning weights that are based on much lower resolutions are probably just not compatible to XL. And how would that work in the first place? As far as I can tell it is not even possible to merge XL models. At least last I tried it wanted to use 64TB ram for that operation.\n\nMaybe I am wrong, so far I have decided not to do much with XL since I am limited to 12GB Vram and there are more than enough very well pre-trained 1.5 models that serve as a good base in the range of image sizes for training. I would assume that training on those resolutions with an XL model would just drag the overall quality.",
        "Hey there!  I am having the same issue.  I have checked into a lot of different sources and there doesn't seem to be a straightforward answer.  I am wondering if this is even a necessary component or something that doesn't really change the game for lora training specifically.  Would love to see what other folks think about this.",
        "Look like a great commit... I hope it will not cause issues for current users... but I will accept it. I don't use docker so I can't really test it... fingers crossed.",
        "The application try to cater to every possible options for every possible LoRA modes\u2026 and as a result create a huge list of irrelevant parameters for types of LoRA that have no relation to them. Not pretty but only way I have coded it so far.\n\nAs far as combining basic and advanced in one page\u2026 it actually used to be like that and I split them apart because some users were complaining about the long list\u2026 I guess it does not please everyone. Not sure I want to work in an option to show them back in one page at this point.",
        "@Ki-wimon I have changed the GUI integration part. You may want to sync your local branch with the dev branch to pull them in. Review the new GUI layout and let me know what you think.",
        "I am getting this as well and I think that it is as Symbio is saying, can't merge lokr style Lora with classical Lora!",
        "Hi, I have but submodules add a level of command complexity for users when cloning and updating the main repo.\n\nTo fix that more coding and handling need to be done using code similar to what I have to use to handle but with different syntax.\n\noverall I prefer to keep the user interaction with git as simple as possible and manage the cloning of the sub repo in case\u2026 unless you can convince doing submodules can be simpler for users and code update of as-script perspective.\n\nMy past experience with submodules has been hell and I have avoided it ever since.",
        "Hello @Jerrisk,\n\nI would really like to merge your PR but it is updating the code that kohya_ss is creating. I just happen to copy his code in my repo when he update his... so I suggest you open a PR directly in his repo and once he merge your change in his main branch I will pull it down in my repo.\n\nhttps://github.com/kohya-ss/sd-scripts/pulls\n\nThe current setup of my repo is weird... I know... there is a lot of history behind why it is what iot is ;-) Maybe one week-end I will study how I could refactor it to just refer to his repo for his code and keep mine clean of needing to copy his files... Something for future me to look into.\n",
        "Well, I've tried your new setup.bat that I think it's crystal clear.... Thanks a lot. \nBut...\n\nI've tried to: \n1. clean the venv\n2. install with torch 2.0\n3. install cuDNN\n4. run the gui\n5. train a lora....\n\nBut then, after it started to train, i get this (see file in attachement) in the cmd windows. It was like that the last time I've tried to use with torch 2.  I insist on the following fact: it used to work before with torch2 ;)\n\nThanks a lot for your help.\n[Kohya torch 2 error.txt](https://github.com/bmaltais/kohya_ss/files/11651041/Kohya.torch.2.error.txt)\n\nI'll try a clean install from scratch with torch 1\n\n\n\n",
        "Nice idea. I see you did not add the change to the textual_inversion_gui.py file. Is it because that trainer does not support the option?",
        "Hello @Cauldrath, thank you for the PR. I'm uncertain about merging Kohya's commits with mine, as I would prefer to keep them separate. This would help maintain focus on the GUI-related work. Could you please share the benefits you envision from merging these commits?",
        "I thought about defaulting to what branch we are on for development purposes, but I think that would confuse new users. I think we should stick to the main branch by default , so we don't mess up any new users. I can switch that if you think the current checked out branch would be better.\n\nEdit: On second thought, I'm going to implement that for pre-existing installs. It shouldn't affect new users since they'd be on stable anyway and dev users like us would just be confused when it switches to master."
    ],
    "F3": [
        "Did most of my training in a nearly year old Kohya version, so I can't say this didn't change in newer ones, but I didn't experienced this issue then. I've recreated some loras a couple of times without any changes in them, and I did change output folders between sessions.\n\nCan you post your full settings?\n\nBesides Kohya version, one thing that definitively changes the training output is torch/xformers/cuda version, so you need to ensure you're using the same Kohya _and_ packages versions to reproduce a lora.",
        "This should also resolve https://github.com/bmaltais/kohya_ss/issues/1565",
        "Hello @Jerrisk,\n\nI would really like to merge your PR but it is updating the code that kohya_ss is creating. I just happen to copy his code in my repo when he update his... so I suggest you open a PR directly in his repo and once he merge your change in his main branch I will pull it down in my repo.\n\nhttps://github.com/kohya-ss/sd-scripts/pulls\n\nThe current setup of my repo is weird... I know... there is a lot of history behind why it is what iot is ;-) Maybe one week-end I will study how I could refactor it to just refer to his repo for his code and keep mine clean of needing to copy his files... Something for future me to look into.\n",
        "Hello @larsupb \n\n1st, many thanks for taking the time to submit this PR. Unfortunatly it is changing a file that is own and maintained by kohya in his repo. I clone hios code into mine and use it along with the GUI. May I suggest you create a seperate PR with kohya at https://github.com/kohya-ss/sd-scripts and once he merge it in his code I could then merge your change to the GUI?\n\nMany thanks,\n\nBernard",
        "> I don't understand what this is supposed to fix. If you select custom in the presets list it should show the fields, does it not?\n> \n> The new version of the GUI is intentionally hiding them unless you want to use a custom model. It certainly change the original behavior but selecting custom in the list will bring the fields up.\n\nI think the issue was here that selecting the `Custom` dropdown option did nothing, crashed, or it didn't appear in the dropdown (ref #1199), but after cloning fresh again and reinstalling, it works as expected for me now, so closing this.",
        "I switched it to Adam and it worked fine. Still learning about all of this, so I had no idea what the different optimizers did.",
        "> AdamW\n\nyes better than that. i did total over 120 full trainings . all for SDXL DreamBooth ",
        "I'm on linux so I'm not 100% sure about windows. But I guess you have to edit this file:\nhttps://github.com/bmaltais/kohya_ss/blob/master/setup.bat#L12\nto point to your python3.10.exe or whatever.",
        "I will bump it in the individual files instead. Thank's for raising this. I will close the PR and implement directly in dev2 in seperate files.",
        "> git clone **--recursive** https://github.com/bmaltais/kohya_ss.git\n\nSo doing exactly what i did? I said it worked. The problem is the getting it from the release page. \n\nFar as i can see it grabs it when you run the installer, so recursive should not be needed the way the gui is set up.",
        "Hummm, when I try running the code I get:\n\n```\nTraceback (most recent call last):\n  File \"D:\\kohya_ss\\train_network.py\", line 507, in <module>\n    train(args)\n  File \"D:\\kohya_ss\\train_network.py\", line 129, in train\n    network = network_module.create_network(1.0, args.network_dim, args.network_alpha, vae, text_encoder, unet, **net_kwargs)\n  File \"D:\\kohya_ss\\LoCon\\locon\\locon_kohya.py\", line 21, in create_network\n    network = LoRANetwork(\n  File \"D:\\kohya_ss\\LoCon\\locon\\locon_kohya.py\", line 119, in __init__\n    self.unet_loras = create_modules(LoRANetwork.LORA_PREFIX_UNET, unet, LoRANetwork.UNET_TARGET_REPLACE_MODULE)\n  File \"D:\\kohya_ss\\LoCon\\locon\\locon_kohya.py\", line 106, in create_modules\n    lora = LoConModule(lora_name, child_module, self.multiplier, self.conv_lora_dim, self.conv_alpha)\n  File \"D:\\kohya_ss\\LoCon\\locon\\locon.py\", line 26, in __init__\n    self.lora_down = nn.Conv2d(in_dim, lora_dim, k_size, stride, padding, bias=False)\n  File \"D:\\kohya_ss\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 444, in __init__\n    super(Conv2d, self).__init__(\n  File \"D:\\kohya_ss\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 131, in __init__\n    self.weight = Parameter(torch.empty(\nTypeError: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n```",
        "Additionally, I would like to maintain a batch file (.bat) solution for Windows users. The primary reason for this suggestion is that a considerable number of users tend to launch the Graphical User Interface (GUI) or initiate the installation process by double-clicking the .bat file. Unfortunately, this approach is not as effective when using PowerShell scripts (.ps1) due to heightened security measures enforced by the Windows operating system. Consequently, it is advisable to implement a solution that utilizes .bat files for the convenience of Windows users... at the possible price of reduced feature given the limitations of such an old scripting language.\n\nI think the less the .ps1 and .bat file does before handling the torch to python the better it is.\n\nVladimandic appear to only do minimal check using his .bat file and does most of the rest in python. I am good if Windows users have to install python and git as pre-req... \n",
        "ok , with the right cuda (11.8), and the right xformers(0.0.20) , clean install , the good accelerate config , it starts with the GPU this time ",
        "It will run the GUI in the venv 100%. Just think it was the accelerate call that wasn't.",
        "even though the the above process allowed everything to load and the the Browser interface launched, I still received this error:\n21:20:25-110077 INFO     nVidia toolkit detected\n21:20:27-013894 INFO     Torch 2.1.0+cpu.\n21:20:27-013894 WARNING  Torch reports CUDA not available\n21:20:27-014879 INFO     Verifying modules installation status from requirements_windows_torch2.txt...\n21:20:27-015888 INFO     Verifying modules installation status from requirements.txt...\n21:20:27-018458 WARNING  Package wrong version: huggingface-hub 0.17.3 required 0.15.1\n21:20:27-019458 INFO     Installing package: huggingface-hub==0.15.1\n21:20:29-743887 INFO     headless: False\n21:20:29-745885 INFO     Load CSS...\nRunning on local URL:  http://127.0.0.1:7860/\n\nI went through many possible solutions, like configuring CUDA_HOME and installing the transformers to the correct version,I still got the same error.  It now works by finally doing this:\npip uninstall torch\npip install torch==2.1.0+cu118 -f https://download.pytorch.org/whl/torch_stable.html\n\nI now have my gpu\n16:27:04-727945 INFO     nVidia toolkit detected\n16:27:05-621951 INFO     Torch 2.1.0+cu118\n16:27:05-633427 INFO     Torch backend: nVidia CUDA 11.8 cuDNN 8700\n16:27:05-635344 INFO     Torch detected GPU: NVIDIA GeForce RTX 4090 VRAM 24564 Arch (8, 9) Cores 128\n16:27:05-636364 INFO     Verifying modules installation status from requirements_windows_torch2.txt...\n16:27:05-637356 INFO     Verifying modules installation status from requirements.txt...\n16:27:07-932207 INFO     headless: False\n16:27:07-934214 INFO     Load CSS...\nRunning on local URL:  http://127.0.0.1:7860\n\nHope this helps others\n\n",
        "Just a note if you're coming in to test, I'm breaking setup.ps1 right now because of the way I have to test this involving committing broken code. Changing things up so that if you ever change Python versions, you just bump a version number line and everything else \"just happens\".",
        "Humm, I am sorry. I guess my comment was not clear. It is best if you can transform your utility to be like the caption.py under the tools directory. If you look at it you will see it can take arguments from the user to specify the image folder, the file_pattern, etc, etc. This is how I envision your utility to be like to it can be used in a similar manner...\n\nhttps://github.com/bmaltais/kohya_ss/blob/master/tools/caption.py\n\nI am not sure adding it to the GUI will really be necessary as this is something you would run once on a folder full of image to create colocated caption files... not something you would need to run often.\n",
        "Try deleting the kohya_ss folder and start the installation from scratch. Something does not appear to be installed properly.",
        "Can you try running `setup-3.10.bat`",
        "It's because you're trying to use this: scale_parameter=False relative_step=False warmup_init=False",
        "Thank you for the submision. Look good. I will merge to dev2 and release this weekend.",
        "Try the laest build, should do OK with buttons now.\n",
        "When I tried, the commands as generated by python don't work on linux. I'll combine this with https://github.com/bmaltais/kohya_ss/pull/272 and update the PR.",
        "I made as much of it work with ubuntu as I could test at the moment. It should in theory still work on windows, but I'm unable to test that.",
        "Same here - I updated (git pull) Kohya_ss yesterday (to version 21.8.0) and the problem come up with a LoRA I trained today. Older LoRAs (trained with older verisons of Kohya_ss) work OK.  I think the issue is that it looks like Automatic1111 only uses some weights of the LoRA but not all so the image is mangled  .... I am wondering if it's worth trying to downgrade to - say - 21.7.5?\n\n________________________________\nFrom: zadokov ***@***.***>\nSent: Tuesday, July 11, 2023 3:25 PM\nTo: bmaltais/kohya_ss ***@***.***>\nCc: appayama11 ***@***.***>; Comment ***@***.***>\nSubject: Re: [bmaltais/kohya_ss] After updating installation I get \"the model may not be trained by `sd-scripts`.\" for Lora model in Automatic1111 (Discussion #1150)\n\n\nfirst all started when I updated kohya_ss to the latest version (even tried a fresh install from the latest version which has the same problem. If you go 'git pull' you'll get latest version.\nWith that version the training works OK and the sample images are generated ok. However when you copy the Lora file to stable diffusion and use it, you'll see in the log that it fails to match the keys (although SD generates the images). I think it uses partial data due to some mismatch.\n\n\u2014\n\nReply to this email directly, view it on GitHub<https://github.com/bmaltais/kohya_ss/discussions/1150#discussioncomment-6417808>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/BBGKK6VWN7OYW5EA3K4BQQTXPVV4ZANCNFSM6AAAAAA2EDXEOU>.\n\nYou are receiving this because you commented.Message ID: ***@***.***>\n"
    ],
    "F4": [
        "> Closing since this is not for code authored in my repo but rather in sd-scripts repo\n\nThanks for pointing that out. Have done a PR over at kohya-ss/sd-scripts instead",
        "Is it possible this has been missing all along? I must have forgotten to add it in there... not one complained about it missing ;-)\n\nOK, I will merge the change as it appear to add missing parameters...",
        "SOLVED: the problem was not Kohya and an update, but due to incorrect settings and changes made unintentionally.\nNow I was able to go back to training as before \ud83d\ude01",
        "Sorry... I don't know what the bench folder is \ud83d\ude05",
        "Yeah\u2026 I think a new flag will be required to hide it\u2026 I need to think of the best way to implement it\u2026 but it was supposed to not display if tensorflow was missing\u2026 not sure why this did not work.",
        "> Didn't want to post in Issues, since I don't think it's a bug and more a user error with some settings. I trained something, tried it in Automatic and get the following error. I also post here instead of Automatic, since other Lora's work, so it most likely has something to do with my trained Lora\n> \n> ```\n> Traceback (most recent call last):\n>   File \"E:\\AI\\AI-webui\\extensions-builtin\\Lora\\lora.py\", line 253, in load_loras\n>     lora = load_lora(name, lora_on_disk)\n>   File \"E:\\AI\\AI-webui\\extensions-builtin\\Lora\\lora.py\", line 172, in load_lora\n>     key_diffusers_without_lora_parts, lora_key = key_diffusers.split(\".\", 1)\n> ValueError: not enough values to unpack (expected 2, got 1)\n> ```\n> \n> And then the image generation likely ignores the Lora (as it does not look anything like the trained images). I basically just want to figure out what settings to look at, so anyone got an idea please?\n\nI am stupid. I set everything in the correct Lora tab settings, but when i hit \"Create model\" i was in the regular model tab...\n",
        "I am really sorry. I wanten to fix the merge issues but ended-up merging into dev... This closed the draft PR you had created and there is no way to re-open it. Really sorry about that. But since it was a draft I reverted the merge to dev.",
        "Sorry for the duplicate",
        "> Is it possible this has been missing all along? I must have forgotten to add it in there... not one complained about it missing ;-)\n\nIt was missing in the master branch. It's already present and working in dev before.",
        "ls\n\n> > Yeah\u2026 I think a new flag will be required to hide it\u2026 I need to think of the best way to implement it\u2026 but it was supposed to not display if tensorflow was missing\u2026 not sure why this did not work.\n> \n> Because visibility **or** self.headless\n> \n> https://github.com/bmaltais/kohya_ss/blob/059afdbf2cc41e568cb7797cf582e02f80f5e667/kohya_gui/class_tensorboard.py#L114-L119\n\nI will add a check for tensorflow... I must have broken the logic when I implemented the two button visibility  in headless.",
        "> @evanheckert Thumbs down me all you like, but I own a 4090, and I have done the testing and the truth hurts. Thumbs down the messenger for telling you straight up facts, else it shows you are biased on a good day.\n> \n> What is your major malfunction with what I said? You have a 4090 too and are so upset you just had to thumbs down the lone voice in the Jensen woods? smh.\n\nI missed the context from your previous post. It sounded like someone just dissing on free open source software without trying to help identify and resolve the problem, which always grinds my gears.\n\nI'll retract my thumbs down, now that I see you've shared a lot of information about your issue.",
        "> Try Dreambooth then, using captions and without reg images. If it still doesn't work, I'm sorry, but to create an SDXL ckpt without taking 15,000 hours, you'll need more VRAM.\n\nThank you, I will try!",
        "Since I merged to dev you will need to re-open the PR... should have tested before merging...\n\nActually I created a branch called LoCon. You can open new PR to in if code need to be changed to address the error above... but perhaps it is related to code change in your repo.",
        "> > I have than upgraded from py 3.10.11 to python 3.11.0 which also still prints the same error.\n> \n> You'll need to keep using python 3.10.11 because kohya_ss has a version check which blocks other python versions. I'm sure that'll change in the future, but 3.10 is a must for now.\n> \n> If you've created the venv using python 3.11, I highly recommend deleting the venv folder. It will be created again when you run setup.bat\n> \n> > Now I've tried to run `venv\\scripts\\pip.exe --prefer-binary matplotlib` like you said, but this also just prints a new error :/ `no such option: --prefer-binary`\n> \n> My bad, I forgot to put the word \"install\" in the command. This should be correct: `venv\\scripts\\pip.exe install --prefer-binary matplotlib`\n> \n> So just run setup.bat, leave it and run the command above, and if that worked then do setup option 1.\n\nOh dear god, what a long journey :D\nSo I deleted the venv folder, installed py 3.10.11 again, used your prompt and started the setup.\nYou were completly right, that solved the issue, thank you so much!\n\nHowever, after starting khoya I got an error saying my install folder contains too many spaces^^\nSo I moved everything, deleted the venv again and started all over. \nNow I am downloading everything for the third time at around 1mbit/s (I live in a very rural area^^). \nI guess this will take a while, but I am sure everything will succeed now.\n\nAgain, thank you so much! I am convinced I would NEVER have solved this alone.",
        "Ah, I understand you meant. Sorry, it was my mistake to add other lines to change. Only I wanted to add was the version of scipy.",
        "@Galunid @bmaltais I admit this got a bit out of hand, but now this install script should install just about anywhere in any condition. Def needs runpod testing though because all I tried to do was fit the above script into our new script and make it configurable.\n\nRunpod workflow should look like runpod pre-setup -> normal Linux OS setup that applies to everyone (so you'd hit the Ubuntu or Redhat clause there) -> run pod post-setup -> auto call gui.sh. And it should work even if you have your own private fork of kohya_ss or anything like that.\n\nRun it locally as `setup.sh --help or -h` for the options.",
        "> When `cache_text_encoder_outputs` is enabled, the training script will fail with a data type error. That error didn't happen in v24.1.4 ... it looks like v24.1.5 pulled in a dev branch of sd-scripts because I see SD3 related code in there.\n\nHumm... Perhaps I commited the wrong sd-scripts branch in my last dev branch commit... LE me fix that...",
        "@jstayco Look like github won't allow to reopen merged PR... lame... sorry about that... can you create a new PR for the MacOS fix? I think it is the only option.",
        "I fucked up with the branch \ud83d\udca9. But now it's the right one.",
        "Confirmed 0.40.0 gives really bad results as you said, sorry about this pull request, looks like I will have to downgrade CUDA to make it work with 0.35.0",
        "Merged to the dev branch. Give it a test. I have started to make significant code improvement in dev... hoping I have not broken anything in the process... but I added a new feature... support for custom GUI config via config.toml",
        "Haha, yeah. Silly mistake; fixed!",
        "Hi, thank you for your answer. Yes, I have run setup.bat several times and also deleted the folder several times.\n\nMay I simply copy the lines from my CMD here. You might discover something that I'm overlooking. I am a complete layman, sorry.\n\nMicrosoft Windows [Version 10.0.19045.2846]\n(c) Microsoft Corporation. Alle Rechte vorbehalten.\n\nD:\\Super Stable Deffusion 2.0\\Kohya>git clone https://github.com/bmaltais/kohya_ss.git\nCloning into 'kohya_ss'...\nremote: Enumerating objects: 4657, done.\nremote: Counting objects: 100% (471/471), done.\nremote: Compressing objects: 100% (120/120), done.\nremote: Total 4657 (delta 366), reused 438 (delta 350), pack-reused 4186\nReceiving objects: 100% (4657/4657), 4.23 MiB | 2.88 MiB/s, done.\nResolving deltas: 100% (3181/3181), done.\n\nD:\\Super Stable Deffusion 2.0\\Kohya>cd kohya_ss\n\nD:\\Super Stable Deffusion 2.0\\Kohya\\kohya_ss>setup.bat\nWarning: Python version 3.10.9 is recommended.\nDo you want to uninstall previous versions of torch and associated files before installing?\n[1] - Yes\n[2] - No\nEnter your choice (1 or 2): 1\nWARNING: Skipping xformers as it is not installed.\nWARNING: Skipping torch as it is not installed.\nWARNING: Skipping torchvision as it is not installed.\nPlease choose the version of torch you want to install:\n[1] - v1 (torch 1.12.1)\n[2] - v2 (torch 2.0.0)\nEnter your choice (1 or 2): 1\nLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\nERROR: Could not find a version that satisfies the requirement torch==1.12.1+cu116 (from versions: 2.0.0)\nERROR: No matching distribution found for torch==1.12.1+cu116\n\n[notice] A new release of pip available: 22.3.1 -> 23.1\n[notice] To update, run: python.exe -m pip install --upgrade pip\nIgnoring gradio: markers 'sys_platform == \"darwin\"' don't match your environment\nIgnoring tensorboard: markers 'sys_platform == \"darwin\"' don't match your environment\nIgnoring huggingface-hub: markers 'sys_platform == \"darwin\"' don't match your environment\nProcessing d:\\super stable deffusion 2.0\\kohya\\kohya_ss\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nCollecting accelerate==0.15.0\n  Using cached accelerate-0.15.0-py3-none-any.whl (191 kB)\nCollecting albumentations==1.3.0\n  Using cached albumentations-1.3.0-py3-none-any.whl (123 kB)\nCollecting altair==4.2.2\n  Using cached altair-4.2.2-py3-none-any.whl (813 kB)\nCollecting bitsandbytes==0.35.0\n  Using cached bitsandbytes-0.35.0-py3-none-any.whl (62.5 MB)\nCollecting dadaptation==1.5\n  Using cached dadaptation-1.5.tar.gz (8.3 kB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nCollecting diffusers[torch]==0.10.2\n  Using cached diffusers-0.10.2-py3-none-any.whl (503 kB)\nCollecting easygui==0.98.3\n  Using cached easygui-0.98.3-py2.py3-none-any.whl (92 kB)\nCollecting einops==0.6.0\n  Using cached einops-0.6.0-py3-none-any.whl (41 kB)\nCollecting ftfy==6.1.1\n  Using cached ftfy-6.1.1-py3-none-any.whl (53 kB)\nCollecting gradio==3.27.0\n  Using cached gradio-3.27.0-py3-none-any.whl (17.3 MB)\nCollecting lion-pytorch==0.0.6\n  Using cached lion_pytorch-0.0.6-py3-none-any.whl (4.2 kB)\nCollecting opencv-python==4.7.0.68\n  Using cached opencv_python-4.7.0.68-cp37-abi3-win_amd64.whl (38.2 MB)\nCollecting pytorch-lightning==1.9.0\n  Using cached pytorch_lightning-1.9.0-py3-none-any.whl (825 kB)\nCollecting safetensors==0.2.6\n  Using cached safetensors-0.2.6-cp311-cp311-win_amd64.whl (268 kB)\nCollecting tensorboard==2.10.1\n  Using cached tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\nCollecting tk==0.1.0\n  Using cached tk-0.1.0-py3-none-any.whl (3.9 kB)\nCollecting toml==0.10.2\n  Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\nCollecting transformers==4.26.0\n  Using cached transformers-4.26.0-py3-none-any.whl (6.3 MB)\nCollecting voluptuous==0.13.1\n  Using cached voluptuous-0.13.1-py3-none-any.whl (29 kB)\nCollecting fairscale==0.4.13\n  Using cached fairscale-0.4.13.tar.gz (266 kB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Installing backend dependencies ... done\n  Preparing metadata (pyproject.toml) ... done\nCollecting requests==2.28.2\n  Using cached requests-2.28.2-py3-none-any.whl (62 kB)\nCollecting timm==0.6.12\n  Using cached timm-0.6.12-py3-none-any.whl (549 kB)\nCollecting huggingface-hub==0.13.3\n  Using cached huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\nERROR: Could not find a version that satisfies the requirement tensorflow==2.10.1 (from versions: 2.12.0rc0, 2.12.0rc1, 2.12.0)\nERROR: No matching distribution found for tensorflow==2.10.1\n\n[notice] A new release of pip available: 22.3.1 -> 23.1\n[notice] To update, run: python.exe -m pip install --upgrade pip\nERROR: xformers-0.0.14.dev0-cp310-cp310-win_amd64.whl is not a supported wheel on this platform.\n\n[notice] A new release of pip available: 22.3.1 -> 23.1\n[notice] To update, run: python.exe -m pip install --upgrade pip\n.\\bitsandbytes_windows\\libbitsandbytes_cpu.dll\nDas System kann den angegebenen Pfad nicht finden.\n.\\bitsandbytes_windows\\libbitsandbytes_cuda116.dll\nDas System kann den angegebenen Pfad nicht finden.\n        0 Datei(en) kopiert.\nDas System kann den angegebenen Pfad nicht finden.\n        0 Datei(en) kopiert.\nDas System kann den angegebenen Pfad nicht finden.\n        0 Datei(en) kopiert.\nDer Befehl \"accelerate\" ist entweder falsch geschrieben oder\nkonnte nicht gefunden werden.\n\n(venv) D:\\Super Stable Deffusion 2.0\\Kohya\\kohya_ss>\n",
        "Hello @godlockin This look like a great PR... but some of it is changing sd-scripts files created and maintained by kohya_ss in his repo. I suggest this PR be changed to only keep the files pertinant to my repo and to open a PR for the sd-scripts files on the kohya_ss repo: https://github.com/kohya-ss/sd-scripts\n\nOnce he has merged the changes in his repo I will then be able to merge them in mine and then apply your modified PR on top.\n\nDoes it make sense? Sorry for the confusion my use of kohya_ss sd-scripts can induce. I only wrak the sd-scripts with a GUI... but I don't maintain the sd-script code myself.",
        "Darn, copy paste issue\u2026 did not notice as I moved code from my Linux test env to my main code commit env."
    ],
    "F5": [
        "Interesting PR. I will try it and see how it look and work. As you said it will probably introduce some confusion if a user create a Locon KoRA using the UI and the utilities choke on it... Wonder what is best for the implementation",
        "I already implemented it dev... One can pass a parameter when starting the gui by setting an environment variable:\n\n```python\nself.tensorboard_port = os.environ.get(\n            \"TENSORBOARD_PORT\", self.DEFAULT_TENSORBOARD_PORT\n        )\n```\n\nto change the tensorboard port... maybe this is enough?\n\nI am trying to make the GUI as convenient as possible so it can be done from there also. I understand there are other methods... but having it there allow users to stay in the GUI for most everything.",
        "As far as torch Kohya only officially support v1... But I offered v2 as some users have been asking for it. I think we should default to 1 and allow users to install v2 if they want to try it out.",
        "Hello @hopl1t \n\nthis look like a great pr but it is modifying kohya sd-scripts files I clone to my repo. May I suggest you create a copy of this PR directly on his repo? When he merge the change I will then get the update when I pull his code into my repo.",
        "I'm looking at the runpod stuff and it seems like it's just a kubernetes-type service allowing access to GPUs. If that's the case, I think you're better off modifying a DOCKERFILE to do the steps you wanted than to try to use a Bash script to do so. I can also start that up for you, but that would have to be a bit later as I'm focusing on macOS efforts for this go around. I advise keeping the runpod out of the bash setup scripts for now though if I'm correct about the docker container story.\n\nThis is probably our starting point for that official kohya_ss docker image: https://hub.docker.com/r/runpod/pytorch\n\nEdit: After looking into a bit further, yes, a proper DOCKERFILE is the answer for that.  Per the runpod FAQ:\n```You can run any docker container available on any publicly reachable container registry. If you are not well versed in containers, we recommend sticking with the default run templates like our RunPod PyTorch template. However, if you know what you are doing, you can do a lot more!```",
        "Actually, there are some good bits in your PR that I could merge directly in dev2... like the localization, typos fix, SDXL 1.0 instead of 0.9, etc.\n\nI feel it might be best to split the PR into two... One for the requirements update that might not be easy to accept given it is moving things in a territory where kohya_ss code may no longer bu supported... and one for the other things that are more bening.\n\nWhat do you think @sammcj ? Perhaps a new branch with the updated requirements could be maintained in the repo to properly test and figure impact on existing users?\n\nCould you split the PR into two?",
        "Hello @Cauldrath, thank you for the PR. I'm uncertain about merging Kohya's commits with mine, as I would prefer to keep them separate. This would help maintain focus on the GUI-related work. Could you please share the benefits you envision from merging these commits?",
        "OK, before I merge, I wonder if you would feel OK to adapt the code to leverage the central common_gui.py implementation vs carrying the config gradio block in each trainer GUI. I know, it is almost as much work given the sprawling of variables in each any way... but if one needed to change the gradio elements in the future this allow to make the change in only one file vs 4.\n\nLet me know if you can tackle this. Otherwise I might refactor the code to implement it using the common_gui.ps in the library folder in the future.",
        "Hello @larsupb \n\n1st, many thanks for taking the time to submit this PR. Unfortunatly it is changing a file that is own and maintained by kohya in his repo. I clone hios code into mine and use it along with the GUI. May I suggest you create a seperate PR with kohya at https://github.com/kohya-ss/sd-scripts and once he merge it in his code I could then merge your change to the GUI?\n\nMany thanks,\n\nBernard",
        "I thought about defaulting to what branch we are on for development purposes, but I think that would confuse new users. I think we should stick to the main branch by default , so we don't mess up any new users. I can switch that if you think the current checked out branch would be better.\n\nEdit: On second thought, I'm going to implement that for pre-existing installs. It shouldn't affect new users since they'd be on stable anyway and dev users like us would just be confused when it switches to master.",
        "Made a tutorial for RunPod\n\nAlso made a pull request i hope you accept it \n\nhttps://github.com/bmaltais/kohya_ss/pull/814\n\n33.) Kohya Web UI - RunPod - Paid\n\n[**How To Install And Use Kohya LoRA GUI / Web UI on RunPod IO With Stable Diffusion & Automatic1111**](https://youtu.be/3uzCNrQao3o)\n\n[![image](https://github.com/FurkanGozukara/Stable-Diffusion/assets/19240467/0c9c3f7d-c308-4793-b790-999fdc271372)](https://youtu.be/3uzCNrQao3o)",
        "> One more question while I have the attention of the masters: about the presets in KOHYA SS (prodigy, LOHA, LOKR, kudou reira...) is there a place where I can see the results and the best moment/style for every one of these presets?\n\nAll of the different LoRA forms do a different thing and work in a different way. Obviously it would be possible to create a standard training dataset and parameters (including seed) and then train in that manner to find example. But the testing range for the lora would have to be quite broad, somewhere like 1000 images in specific sets and types of prompting. But even then you just get general broad approximation on what kind of results you might get from them, there is no guarantee that your dataset would yield those kinds of results. This is even more so when you add captions to your images. (And this is still with the assumption that everything is implemented and working correctly as described in the papers).\n\nI personally just use standard LoRA and it has thus far managed to serve me perfectly. As in I have yet to try to something that I can't make happen because of technical limitations. All my limitations are frankly my own down and fault, generally down to me being (in my opinion) quite bad at prompting and generating images compared to training things. \n\nBut my experience is just that... experience. I have made hundreds of LoRAs and tens of version of each on my humble 4060TI - none that I have really published officially. And just like in my real life when I as an engineer go adjust some cutting system, I quite literally just test values and write down observations. Then make conclusions on what works, what doesn't and what is the desired and undesired property.\n\nFor example when it comes to how many ranks I use: I figured that out just by starting from rank 1 and seeing at what point the LoRA \"starts to work\". Then I worked up until I concluded there was no meaningful benefit, or the functionality became inferior.\n\nThe presets in the GUI are just some one person's opinion about what are the good settings. Other than some specific optimiser related things (like extra arguments to make it work) it largely boils down to opinion. I'm quite sure that if I published my favourite DAdaptAdam preset there it would be just as good and any other's - even though lot of the things in it go against the grain of public opinion.  This is just because I have made so many trial and error runs and read the papers and technical docs.  I consider my method to be _\"purer\"_ in that sense but not best or good. \n\nHonestly. Just choose one and start experimenting... and write down the thing you did and observations relating to it. ",
        "Thanks @synystersocks that worked perfectly. It might be nice to have this as a 'default' SDXL preset. Currently it's unclear what preset to use and I think the one I was using last time was too heavy (or unoptimized) for my machine.\n\n```\n139/2640 [04:55<1:28:28,  2.12s/it, loss=0.119]\n```\n\nOn a RTX 4070.",
        "I am reworking how the GUI is installed. I don't like all the questions and answers steps. I think I will split things in two different setup files. One for torch1 and one for torch2. Requirements are different for both so I will also use torch version specific requirements files.\n\nThe torch one version will use the same modules versions as Kohya to stay as \"pure\" as possible.\n\nTorch2 will be more experimental with newer versions of modules... So more prone to breakage.\n\nI will make the torch1 version the default. I hope this will minimise issues for users.",
        "This look great. This next release will see some major upgrade ;-) I like it. This will be some major rework of the GUI. It will make life easier for many users.",
        "Awesome, will make updates more user freindly if ever needed in the future. I will merge to dev and let it soak until the next release.",
        "Sounds good. Yeah, his only takes a second because he is not doing any checks or repairs at all. Whereas this solution does it on every startup to avoid corrupted venv situations. I can just add a --repair option to all scripts though and figure out a way to detect first runs vs subsequent runs and do it that way. It's still better than the current method of having users use different scripts for different things. For the setup.bat situation? It should still abide by your exact request and it should be invisible to users because it uses all the old arguments + the new ones and has defaults for those not provided. That's the only real addition. It does basically no checks now and just passes the arguments right on through to launcher.py which does all the real work. So the user experience should be the same there. All the setup.ps1 and setup.sh do is check for and install if missing Python, TK, and git. Launcher.py is doing all the \"real work\". This was done on purpose so we'd have a good single file to control the entire cross platform installation and execution story.\n\nFor the logs? I have the configuration files with the values taking precedence in this order: $SCRIPT_DIR/install_config.yml, $USERPROFILE/.kohya_ss/config_install.yml, and $SCRIPT_DIR/config_files/installation/install_config.yml that way you can have user-wide settings, repo-specific settings, and the factory defaults as a fallback. I put the logs in $USERPROFILE/.kohya_ss/logs as a move to start centralizing some pieces like configuration files and logs and start moving to standard locations instead of silo-ing everything off. If you still want me to put the logs in $SCRIPT_DIR/logs, that's an easy change.",
        "Currently, \"network_train_unet_only\" seems to be automatically determined whether to include it or not. In the following code snippet from lora_gui.py, when \"text_encoder_lr\" is 0 and \"unet_lr\" is not 0, it will be automatically added.\n\n```py\nif not (float(text_encoder_lr) == 0) or not (float(unet_lr) == 0):\n    if not (float(text_encoder_lr) == 0) and not (float(unet_lr) == 0):\n        run_cmd += f' --text_encoder_lr={text_encoder_lr}'\n        run_cmd += f' --unet_lr={unet_lr}'\n    elif not (float(text_encoder_lr) == 0):\n        run_cmd += f' --text_encoder_lr={text_encoder_lr}'\n        run_cmd += f' --network_train_text_encoder_only'\n    else:\n        run_cmd += f' --unet_lr={unet_lr}'\n        run_cmd += f' --network_train_unet_only'\n```\n\nThe author of sd-scripts, kohya-ss, provides the following recommendations for training SDXL:\n\n> kohya-ss:\n> Please specify `--network_train_unet_only` if you caching the text encoder outputs.\n> \n> For the second command, if you don't use the option `--cache_text_encoder_outputs`, Text Encoders are on VRAM, and it uses a lot of VRAM. So please add the option (and also add `--network_train_unet_only`).\n> https://github.com/kohya-ss/sd-scripts/issues/661#issuecomment-1643945086\n\nI wonder if the developer of kohya_ss_gui has considered adding an option in the GUI interface for users to decide whether to enable \"network_train_unet_only\"?",
        "No problem at all! I think I'll try to give the Windows install the same features when I feel a bit better about the macOS GUI portions. I'd like to have feature parity across the install story if I can.",
        "I will gladly accept the PR as long as you keep the versions aligned with the sd-scripts author and not use versions that are newer than what kohya officially support.",
        "@bmaltais Looks good! Only thing that's missing is the updated README that you can find here with all those formatting updates and fixes: https://github.com/jstayco/kohya_ss/blob/readme_update/README.md",
        "It is, quite easy be default. There are full settings tutorials for this*. Lowest I have seen people do is 6gb, but some have managed to pull 4gb (by having lots of ram to fall back on). However you don't need to worry too much, since with Nvidia you can easily use some of the RAM also, the system works. There is also the dynamo backends which help with this. Just give it a try. I run 4060 TI 18gb and depending on what I am doing, I can be low as 6 gb and high as 15 gb. I run fp8 training by default regardless.\n\nAlso you don't need high resolutions images to train with. My dataset is a mess of all sort of dimensions for a reason, it helps with the training. My smallest dimension can be low as 384. But generally I don't go above 1024 because I haven't really found any actual benefit for it in my training runs - just slows the process down.\n\n*Keep in mind that all settings are just suggestion. There are no optimal settings for training, every thing you want to do will require some adjusting along with trial and error to get the best output. I don't really do LoRAs to generate with or publish, I just enjoy the puzzle of trying to get a thing to happen.\n\nAlso don't be scared if the training overflows to use RAM side, most of the time it doesn't affect speeds at all and even if it does the difference not really anything to worry about. When you are training with your own setup, it calls for certain amount of patience to begin with. I run training when I am at work, or I do other things.\n\nE: Just to add... Just try stuff. Look guides only enough to get started and then just try stuff. Make notes on what works and doesn't work. Just... Have fun with the process. You will very quickly find things which work, and things that save Vram and wasn't mentioned in some guide. Go forth and have fun!",
        "I think the current instructions are correct for the latest docker versions... the other version you are proposing is being deprecated... It might be better to update the documentation to instruct the users to use the syntax that is pertinant to the version of docker they use?\n\nThe docker compose (with a space) is a newer project to migrate compose to Go with the rest of the docker project. This is the v2 branch of the [docker/compose](https://github.com/docker/compose/tree/v2) repo. It's been first introduced to Docker Desktop users, so docker users on Linux didn't see the command. In addition to migrating to Go, it uses the compose-spec, and part of the rewrite may result in behavior differences.\n\nThe original python project, called docker-compose, aka v1 of docker/compose repo, has now been deprecated and development has moved over to v2. To install the v2 docker compose as a CLI plugin on Linux, supported distribution can now install the docker-compose-plugin package. E.g. on debian, I run apt-get install docker-compose-plugin.",
        "Some of you might have seen me here. I'm not a coder - however I am an engineer of a non-software variety. I can be found to be discussion rather near metaphysical and philosophical aspescts of finetuning. Generally also can be seen going against the \"common accepted knowledge\" since I been able to pull off everything I want thus far with and without: Captions, Regularisation images, Massive dataset of pictures. I don't do anime stuff, and generally I hang around the far borders of strange, odd and \"whether this is possible at all\". Which usually means what I do and can be seen talking about have very little to do with \"making pretty images\" and more about \"does this concept work at all\". \n\nI'm a great fan of documentation with goal of trying to write something down which could useful for others to use. However I keep getting sidetracked with that task. I'm also appreciator of good UI/UX design which is human centric in nature, and I might aswell be wishing for an utopia on that front. Despite being an engineer, I think the biggest flaw of us engineers is to make things that only other engineers can possibly understand. \n\nI am also often found at giving people basic troubleshooting tips, which usually are them misunderstanding something very basic about setting things up... Or running to that one problem that in the year of our lord 2024 you still can't reliably use \"special\" characters - like \u00e4\u00f6\u00e5 - in filenames or paths. ",
        "> One possibility would be to backrev the gradio version to align with a1111... but for now don't bother... I think it is still supported and not throwing a fix about the deprecation...\n> \n> I wish I could start over the GUI code and just \"clone\" the sd-script folder during setup and leverage that for execution for the GUI.\n> \n> Right now both are sort of intertwine and not the best... but it is what it is ;-) When I started coding the GUI kohya did not us git so I was copying his \"notes\" as code in my repo. After we talked a bit more he decided to adopt my gui structure but in his own repo... so there started the mess of code it is ;-)\n> \n> But with the refactoring you have started to do I could see the possibility of totally removing his code from my repo and just \"clone\" a specific commit for use instead of carrying his code base in mine... this would prevent users from attempting to commit updates to kohya's code in my repo...\n\noh!! thank you for the detailed explanation.\n\nyes, that would be one possible way. (+ git submodules or some scripts?)\nanyway, your gui frontend is/are so convenient that many people will use it for a while, I think."
    ],
    "F7": [
        "I really wish these developers would stop changing stuff when it works. This new setup file is a disaster. Before it just worked. Now the dev has tried to be too clever and broke everything ffs",
        "I could post a lot more, but going through issues takes quite a while. If I find them I will post other examples. We can see though in sd-scripts @FurkanGozukara posts this MR to add his videos to the repo: https://github.com/kohya-ss/sd-scripts/pull/736\n\nWhere he claims \n\n> I think this is the most comprehensive ever tutorial made for Kohya\n\nAnd receives 6 downvotes. As far as I know, @FurkanGozukara doesn't even use sd-scripts directly.\n\nI'd agree with @Sj-Si assessment:\n\n> First time I've seen a PR that was literally a self advertisement. Does this not seem a bit pretentious? Not only that but it is the first line in the Tips section. Man, what a laugh.\n\nThis guy doesn't contribute in any way but posts self advertisements in these repos as a way to get clients and patreon subscribers. It would be a different thing if some random person was like \"wow I found this video so helpful, let me add it to the readme\". No, it is pure self advertisement and marketing. ",
        "@nvjob Nonsense!  The template works perfectly fine.  Look at the container logs NOT the system logs, the system logs are completely useless.  The problem here is that you are too impatient and did not wait for the apps to complete syncing to workspace.  There is absolutely nothing wrong with the template.  The problem here is 100% YOU.",
        "What a mess , anyone have one answer to this ? Aint gonna train sdxl for 24 hours on 20 pics, this is horrible.",
        "Let's try and keep things civil here. To summarize my main argument: I don't think it's really appropriate for @FurkanGozukara to be self promoting via open source GitHub repos. There are many issues both in this repo, sd-scripts, and others where he posts videos to his YouTube. He is incredibly spammy on Reddit as well, where you can see he posts his videos on all the ai subreddits constantly. As of the last few months, he has pushed to not only spamming the subreddits but gating basic information behind his Patreon (learning rate, batch size, dim/alpha, pretty much all basic settings). He is also just generally annoying in many random issues posted here and in sd-scripts where he isn't actually contributing but instead is either begging people for their configs, posting links to his videos, or asking when things are going to be merged on issues that were closed. \n\nI recommend that his videos are removed. In the place of his videos I'm sure we can find other videos to post, there are many that describe the same setup. There are also some great articles I've read that get into the setup. I'd be happy to post some links. I don't think in general it makes sense to allow people to self promote in an open source repo like this where so many people go when they first want to get setup training SD. I think that's true in particular when their behavior is incredibly spammy across different platforms and they are disingenous in their claims on \"full workflow\" \"no paywall\" etc.\n\nUltimately it is up to @bmaltais to decide what to do here, I just wanted to at least bring the issue to his attention.",
        "same issue here, it seems like many things have changed since this was uploaded. i dont expect this to be updated or answered, once people have it working for themselves they dont bother to update or change anything.",
        "Once again, you misunderstand what I am saying @FurkanGozukara . My claim is not solely about these particular videos but about your behavior in general. I will post some links:\n\nOn reddit you post this: [Comparison Between SDXL Full DreamBooth Training (includes Text Encoder) vs LoRA Training vs LoRA Extraction - Full workflow and details in the comment](https://www.reddit.com/r/StableDiffusion/comments/18nxwt3/comparison_between_sdxl_full_dreambooth_training/) which contains \n[the comment](https://www.reddit.com/r/StableDiffusion/comments/18nxwt3/comment/kednkxg/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button).\n\nIn the post title you claim \"full workflow\". In the comment you say \"I strongly suggest to read it on Medium. Open article - no paywall or anything.\" But here is the thing, there is a paywall: your patreon.\n\nIn searching the [medium article](https://medium.com/@furkangozukara/comparison-between-sdxl-full-dreambooth-training-vs-lora-training-vs-lora-extraction-44ada854f1b9), you don't post relevant configuration of your LoRA or Dreambooth training:\n\n* Learning rate\n* Batch size\n* Optimizer\n* LR scheduler\n* BF16 / FP16\n* Gradient checkpointing or not \n* Gradient accumulation\n* Min SNR Gamma\n\nBasically every relevant piece of config you leave out. So two claims are invalid: that this is the full workflow, and that there is no paywall. To get the config with these basic training settings that you beg everyone for all over GitHub Issues one must pay for your patreon. Or you also offer \"private consulting\". \n\nHere are some other posts where you also don't post your settings [[1]](https://www.reddit.com/r/StableDiffusion/comments/18uh0nn/dreambooth_sdxl_10_of_myself_vs_dalle_3_vs_sdxl/), [[2]](https://www.reddit.com/r/StableDiffusion/comments/18vb3fx/le_tour_jurassic_park_with_stable_diffusion_xl/), [[3]](https://www.reddit.com/r/StableDiffusion/comments/18segjh/generate_photos_of_yourself_in_different_cities/), [[4]](https://www.reddit.com/r/DreamBooth/comments/18myal5/trained_a_new_stable_diffusion_xl_sdxl_base_10/). I could post a lot more but... \n\nIt's honestly a bit hard to navigate your [Reddit history ](https://www.reddit.com/user/CeFurkan/submitted/) because you literally spam so many subreddits so frequently that it's a bit hard to get beyond 30 days without a lot of scrolling. Interestingly though, [here is another one ](https://www.reddit.com/r/StableDiffusion/comments/17qy4hi/stable_diffusion_xl_sdxl_full_dreambooth_model_vs/)that I found from 2 months ago which requires a significant amount of scrolling where you also didn't provide settings. Apparently mods removed this post, not sure why. Probably due to you grifting.\n\nI do recall another past post where you claimed that TE training was broken in sd-scripts. I can't find it now cause I'd have to scroll through 8 billion spam posts on reddit, but I believe this was the relevant sd-scripts issue: https://github.com/kohya-ss/sd-scripts/issues/890\n\nWe can see @FurkanGozukara contributing nothing to the issue but just writing random nonsense while devs and actual issue reporters discuss what is going on. He claimed TE training was completely broken on reddit, but clearly per the issue it was specific to the block LR options.\n\nHere is a really interesting comment that showcases what I am describing: https://github.com/kohya-ss/sd-scripts/issues/890#issuecomment-1780199660\n\n> can you share your command please?\n\nHere he asks for someone to share their command so that he can benefit, yet he is unwilling to provide the same benefit back to the community. Instead his goal is to spam the community as much as possible to drive people to his Patreon where he has gated some of this basic information from his \"learnings\". \n\nThere are a ton of other examples of @FurkanGozukara trolling both this GitHub repo, sd-scripts, as well as many other issues basically begging people for configs and contributing nothing to the actual conversation at hand. Then when it comes his turn to share with the community some basic settings (learning rate, batch size, etc) he claims he is sharing \"free\" \"full workflow\" \"no paywall\" medium articles and posts where he doesn't actually share anything important. How can we possibly evaluate your comparison of LoRA training to full fine tune if you can't even post a learning rate for each or other basic settings? ",
        "I already explained why it is not possible few months ago on some issue ==\nPlease search all the issue before tagging me for any kind of these ideas.",
        ">For the majority of these 6 months I've been using regularization images. When training styles. Which people swear up and down you must not. \n\nI don't use reg images, I don't even use captions unless I struggle to get the focus. The original training papers and such don't even mention regulation. Since the method and math originates from the papers - I am confident that random people making anime titties aren't any more knowledgeable. \n\n>A lot of advice on regularization and even training is bullshit. Really, it's a sea of bullshit out there and people keep repeating stuff they heard somewhere else without looking into it as if they're proselytizing their new religion.\nI find that all community sources, especially on social media/youtube and such with flashy titles and \" _This is how you can make realistic professional amazing art images FOR FREE! Automatic1111 RunPod AnimeTits_ \" flat out don't know what they are doing. And having watched few out of curiosity I know that for sure. \n\nWhat tilts me a fair bit is that: Beyond some of the more arcane aspects, all the details and information on how these things work is on the papers, githubs or other similar sources and documentation. Obviously you can't really know whether something is actually implemented and working correctly.\n\nBy bar for whether a source is worth a damn, is really whether they cite the sources correctly. No I don't mean \"link to where I read this\" but citing and sourcing at least in manner that would pass in academic or professional setting. \n\nBut when it comes to the discussion or reg images. If you get good quality results that meet the criteria you set, without them. Do you actually need them? Because some issue people say can and should be solved with reg images, I have solved by switching the model I train from - only going to base SDXL if I am desperate. Currently the best I have come across and what I use is FluentlyXL, no idea why but it just gives me cleaner results.\n\n>You mean like the passage of time influences training results and generation? I have been training loras for over 6 months and have not experienced this at all. But these two extra factors I mentioned absolutely do influence training and generation.\n\nNot the \"passage of time\" per se. Since all the random components in the training are generated from clock, which is fiddly piece of tech. Minute alternations on what the time is affect things.  Nothing in these are 100% deterministic - we are talking about something that is inherently statistical afterall. But the time is never the reason why something works or doesn't work. It is just... tiny little things. But tiny little things can cause noticeable differences, but not to degree where it is \"did work\" or \"didn't work\".\n\nLike I talk alot about this stuff, since it is my current obsessive hobby (well training is. I am quite bad at generating, I just like the puzzle of training). And I always wonder whether I am doing \"something wrong\" to what other people are doing, because I can get things done just fine without captions, reg images, or... other bullshit reddit and whatever deems absolutely mandatory for getting good quality results. I don't do any of that and I get good results. If I can't get something to wrong I deep dive to documentation and papers, and the answers are generally just there in clear print. ",
        "What up devs ? what was the last working comit ? ITs 2 weeks since this repo is unuseable ",
        "I agree with OP. Furkan has been spamming his videos on TONS of repositories and sites (reddit, discords, etc) so much that its annoying.",
        "Your newest videos and posts on reddit hide the important training parameters like learning rate. I realize these _older_ videos perhaps don't hide those values, but they essentially just act as promotion material for you and will cause someone to think \"oh let me look at his latest videos since he's probably learned some things since then\".\n\nI only reference Aitrepeneur because he includes all values in his videos and his paywall is just for convenience scripts. You in contrast hide the learning rate in your latest posts and videos, along with other important settings. So you are essentially grifting off the community and the information other people share freely with you and then refusing to share your learnings back with the community.",
        "OP: \"I'm so entitled that when the **free** software I use breaks after an update, I'm allowed to insult the developers and not provide real feedback or solutions.\"\n\nBruh. This is a completely brand new technology and an incredibly small fraction of the entire Earth's population knows how to write code and develop code specifically for it.\n\nIf you want something that works, go learn computer science including all the math, AI, Python, and all sorts of other stuff and write your own damn SD training app.",
        "Now, Kohya ss is really bad. It is too slow to make lora.\n\nBefore 2023-06-03 I still can use Torch 2 and cudnn, It so fast, \n\nBut now It's dead...So sad",
        "i have given up\n\n1) it took 36 hours to create only one single Lora,  instead of 10 Loras as set in the prefs , only one, which then did not work.\n\ni use now Draw Things app on Mac from the apple store,  to create loras now, which can be easily made and easily exported and work somewhat ok in auto1111 or in draw things. or anywhere else.\n\ni guess the fact that there is no one helping nor fixing nor improving how to make Koya SS work on a mac,... is the trigger to walk away from this otherwise interesting app. I am sure i am one of many many mac users....who just gave up on Kohya\n\nStrange thing is that on this entire planet only people from the advertising world would have real life application for this Kohya SS , and 100% of all creative directors art directors from such ad agencies only exclusively use mac computers. No one would eve touch a PC.\n\n....and KOHYA ss is exclusively made for PC....so for people who do this for fun, but never ever for any real life applications.\n\nuntil this KOHYA SS is fixed to work on macs easily and fast, by using its GPU (and so on) correctly , this app will eventually be forgotten and vanish.\n\nanyway...\n\nThanks for replying....\n\nAkos\n\n\n\n\n\nOn 31 Jul 2024, at 13:30, elistys ***@***.***> wrote:\n\n\n\"after 3 weeks of no results.\"\n\nWhat were you having trouble with?\n\nI've been trying for 6 hours using the thread below as a reference, but it doesn't work. I haven't tried your settings yet.\n\n#1248 <https://github.com/bmaltais/kohya_ss/issues/1248>\n\u2014\nReply to this email directly, view it on GitHub <https://github.com/bmaltais/kohya_ss/discussions/2570#discussioncomment-10200438>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/ANLIMFK6375DKU4TH7T3TPTZPDDFZAVCNFSM6AAAAABI5TPEDWVHI2DSMVQWIX3LMV43URDJONRXK43TNFXW4Q3PNVWWK3TUHMYTAMRQGA2DGOA>.\nYou are receiving this because you authored the thread.\n\n\n",
        "> I'm seeing this also with my 4090, all 24GB of VRAM get used up CUDA usage is at 99% and it's training at 10.2s/it for a 1024x1024 res\n> \n> `accelerate launch --num_cpu_threads_per_process=2 \"./sdxl_train_network.py\" --enable_bucket --min_bucket_reso=256 --max_bucket_reso=2048 --pretrained_model_name_or_path=\"E:/Models/Stable-Diffusion/Checkpoints/SDXL1.0/sd_xl_base_1.0. safetensors\" --train_data_dir=\"E:/Models/Stable-Diffusion/Training/Lora/test\\img\" --resolution=\"1024,1024\" --output_dir=\"E:/Models/Stable-Diffusion/Training/Lora/test\\model\" --logging_dir=\"E:/Models/Stable-Diffusion/Training/Lora/test\\log\" --network_alpha=\"1\" --save_model_as=safetensors --network_module=networks.lora --text_encoder_lr=0.0001 --unet_lr=0.0001 --network_dim=256 --output_name=\"test\" --lr_scheduler_num_cycles=\"5\" --no_half_vae --learning_rate=\"0.0001\" --lr_scheduler=\"constant\" --train_batch_size=\"1\" --max_train_steps=\"5800\" --save_every_n_epochs=\"1\" --mixed_precision=\"bf16\" --save_precision=\"bf16\" --seed=\"12345\" --caption_extension=\".txt\" --cache_latents --cache_latents_to_disk --optimizer_type=\"Adafactor\" --optimizer_args scale_parameter=False relative_step=False warmup_init=False --max_data_loader_n_workers=\"4\" --bucket_reso_steps=64 --save_state --xformers --bucket_no_upscale --noise_offset=0.0357 --sample_sampler=euler_a --sample_prompts=\"E:/Models/Stable-Diffusion/Training/Lora/test\\model\\sample\\prompt.txt\" --sample_every_n_steps=\"100\"`\n> \n> > Version: v21.8.5\n> > Torch 2.0.1+cu118\n> > Torch detected GPU: NVIDIA GeForce RTX 4090 VRAM 24564 Arch (8, 9) Cores 128\n> \n> Even when it generates sample prompt images it's way slower then in automatic1111,\n> \n> height: 1024 width: 1024 sample_steps: 40 scale: 8.0\n> \n> 28%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 11/40 [00:18<00:49, 1.70s/it]\n> \n> in A1111 i get 10 it/s with that res and eulera at that res\n\nwow this is terrible\n\nif you are my patreon supporter i would like to connect your pc and try to help",
        "> Cant fucking stand anymore\n\nXXXXDDDDD\n- And since I can't stand it anymore, I'll have to throw a tantrum....\n\n\u0421ool strategy! You will change this world, we believe in you, only with you - it will become better\n"
    ],
    "F10": [
        "Naughty for Colab free as we aren't allowed to use WebUI now. :(",
        "Once again, you misunderstand what I am saying @FurkanGozukara . My claim is not solely about these particular videos but about your behavior in general. I will post some links:\n\nOn reddit you post this: [Comparison Between SDXL Full DreamBooth Training (includes Text Encoder) vs LoRA Training vs LoRA Extraction - Full workflow and details in the comment](https://www.reddit.com/r/StableDiffusion/comments/18nxwt3/comparison_between_sdxl_full_dreambooth_training/) which contains \n[the comment](https://www.reddit.com/r/StableDiffusion/comments/18nxwt3/comment/kednkxg/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button).\n\nIn the post title you claim \"full workflow\". In the comment you say \"I strongly suggest to read it on Medium. Open article - no paywall or anything.\" But here is the thing, there is a paywall: your patreon.\n\nIn searching the [medium article](https://medium.com/@furkangozukara/comparison-between-sdxl-full-dreambooth-training-vs-lora-training-vs-lora-extraction-44ada854f1b9), you don't post relevant configuration of your LoRA or Dreambooth training:\n\n* Learning rate\n* Batch size\n* Optimizer\n* LR scheduler\n* BF16 / FP16\n* Gradient checkpointing or not \n* Gradient accumulation\n* Min SNR Gamma\n\nBasically every relevant piece of config you leave out. So two claims are invalid: that this is the full workflow, and that there is no paywall. To get the config with these basic training settings that you beg everyone for all over GitHub Issues one must pay for your patreon. Or you also offer \"private consulting\". \n\nHere are some other posts where you also don't post your settings [[1]](https://www.reddit.com/r/StableDiffusion/comments/18uh0nn/dreambooth_sdxl_10_of_myself_vs_dalle_3_vs_sdxl/), [[2]](https://www.reddit.com/r/StableDiffusion/comments/18vb3fx/le_tour_jurassic_park_with_stable_diffusion_xl/), [[3]](https://www.reddit.com/r/StableDiffusion/comments/18segjh/generate_photos_of_yourself_in_different_cities/), [[4]](https://www.reddit.com/r/DreamBooth/comments/18myal5/trained_a_new_stable_diffusion_xl_sdxl_base_10/). I could post a lot more but... \n\nIt's honestly a bit hard to navigate your [Reddit history ](https://www.reddit.com/user/CeFurkan/submitted/) because you literally spam so many subreddits so frequently that it's a bit hard to get beyond 30 days without a lot of scrolling. Interestingly though, [here is another one ](https://www.reddit.com/r/StableDiffusion/comments/17qy4hi/stable_diffusion_xl_sdxl_full_dreambooth_model_vs/)that I found from 2 months ago which requires a significant amount of scrolling where you also didn't provide settings. Apparently mods removed this post, not sure why. Probably due to you grifting.\n\nI do recall another past post where you claimed that TE training was broken in sd-scripts. I can't find it now cause I'd have to scroll through 8 billion spam posts on reddit, but I believe this was the relevant sd-scripts issue: https://github.com/kohya-ss/sd-scripts/issues/890\n\nWe can see @FurkanGozukara contributing nothing to the issue but just writing random nonsense while devs and actual issue reporters discuss what is going on. He claimed TE training was completely broken on reddit, but clearly per the issue it was specific to the block LR options.\n\nHere is a really interesting comment that showcases what I am describing: https://github.com/kohya-ss/sd-scripts/issues/890#issuecomment-1780199660\n\n> can you share your command please?\n\nHere he asks for someone to share their command so that he can benefit, yet he is unwilling to provide the same benefit back to the community. Instead his goal is to spam the community as much as possible to drive people to his Patreon where he has gated some of this basic information from his \"learnings\". \n\nThere are a ton of other examples of @FurkanGozukara trolling both this GitHub repo, sd-scripts, as well as many other issues basically begging people for configs and contributing nothing to the actual conversation at hand. Then when it comes his turn to share with the community some basic settings (learning rate, batch size, etc) he claims he is sharing \"free\" \"full workflow\" \"no paywall\" medium articles and posts where he doesn't actually share anything important. How can we possibly evaluate your comparison of LoRA training to full fine tune if you can't even post a learning rate for each or other basic settings? ",
        "What a mess , anyone have one answer to this ? Aint gonna train sdxl for 24 hours on 20 pics, this is horrible.",
        "What up devs ? what was the last working comit ? ITs 2 weeks since this repo is unuseable ",
        "All the solutions above don't work. Solutions from other opened threads don't work either. \n\n**Why even make the main instruction for linux when 90% of sd users run windows?** \n\nWhen I run this code, it gives numerous errors and when one error is solved, another pops out. It is a nightmarish experience. Esp having a nice gpu, Jesus.\n",
        "Let's try and keep things civil here. To summarize my main argument: I don't think it's really appropriate for @FurkanGozukara to be self promoting via open source GitHub repos. There are many issues both in this repo, sd-scripts, and others where he posts videos to his YouTube. He is incredibly spammy on Reddit as well, where you can see he posts his videos on all the ai subreddits constantly. As of the last few months, he has pushed to not only spamming the subreddits but gating basic information behind his Patreon (learning rate, batch size, dim/alpha, pretty much all basic settings). He is also just generally annoying in many random issues posted here and in sd-scripts where he isn't actually contributing but instead is either begging people for their configs, posting links to his videos, or asking when things are going to be merged on issues that were closed. \n\nI recommend that his videos are removed. In the place of his videos I'm sure we can find other videos to post, there are many that describe the same setup. There are also some great articles I've read that get into the setup. I'd be happy to post some links. I don't think in general it makes sense to allow people to self promote in an open source repo like this where so many people go when they first want to get setup training SD. I think that's true in particular when their behavior is incredibly spammy across different platforms and they are disingenous in their claims on \"full workflow\" \"no paywall\" etc.\n\nUltimately it is up to @bmaltais to decide what to do here, I just wanted to at least bring the issue to his attention.",
        "this effing tkinter is preventing me from using kohya_ss on runpod for weeks now, \nIm literally pulling out my hair, frustated since I have no coding background, \nall guides and tutorials gave me the same error, and after trying everything from this page, it still gives me the same error : \n\n\n\nValidating that requirements are satisfied.\nAll requirements satisfied.\nTraceback (most recent call last):\n  File \"/workspace/kohya_ss/kohya_gui.py\", line 4, in <module>\n    from dreambooth_gui import dreambooth_tab\n  File \"/workspace/kohya_ss/dreambooth_gui.py\", line 13, in <module>\n    from library.common_gui import (\n  File \"/workspace/kohya_ss/library/common_gui.py\", line 1, in <module>\n    from tkinter import filedialog, Tk\nModuleNotFoundError: No module named 'tkinter'\n(venv) root@a15b90ed32cb:/workspace/kohya_ss# ",
        "Now, what is this?\n\n=============================================================\nModules installed outside the virtual environment were found.\nThis can cause issues. Please review the installed modules.\n\nYou can uninstall all local modules with:\n\ndeactivate\npip freeze > uninstall.txt\npip uninstall -y -r uninstall.txt\n\n=============================================================\n\n12:14:18-662189 INFO     nVidia toolkit detected\n12:14:19-685163 INFO     Torch 1.12.1+cu116\n12:14:19-702673 INFO     Torch backend: nVidia CUDA 11.6 cuDNN 8901\n12:14:19-704670 INFO     Torch detected GPU: NVIDIA GeForce RTX 3080 VRAM 12288 Arch (8, 6) Cores 70\n12:14:19-705672 INFO     Verifying requirements\n12:14:19-751183 WARNING  Package wrong version: huggingface-hub 0.15.1 required 0.13.3\n12:14:19-752182 INFO     Installing package: huggingface-hub==0.13.3\n12:14:26-545070 INFO     headless: False\n12:14:26-549068 INFO     Load CSS...\nRunning on local URL:  http://127.0.0.1:7861\n\nTo create a public link, set `share=True` in `launch()`.\n12:14:50-182032 INFO     Loading config...\n12:14:50-261548 INFO     LoRA type changed...\n12:14:59-552952 INFO     Start training LoRA Kohya LoCon ...\n12:14:59-555950 INFO     Folder 50_EAV P: 46 images found\n12:14:59-556952 INFO     Folder 50_EAV P: 2300 steps\n12:14:59-557950 INFO     Total steps: 2300\n12:14:59-558950 INFO     Train batch size: 2\n12:14:59-559950 INFO     Gradient accumulation steps: 1.0\n12:14:59-560950 INFO     Epoch: 1\n12:14:59-561950 INFO     Regulatization factor: 1\n12:14:59-562949 INFO     max_train_steps (2300 / 2 / 1.0 * 1 * 1) = 1150\n12:14:59-564952 INFO     stop_text_encoder_training = 0\n12:14:59-565951 INFO     lr_warmup_steps = 115\n12:14:59-567952 INFO     accelerate launch --num_cpu_threads_per_process=2 \"train_network.py\"\n                         --pretrained_model_name_or_path=\"E:\\Game\n                         2023\\AIWAIFU\\SD15\\stable-diffusion-webui\\models\\Stable-diffusion\\yesmix_v16Original.safetensors\n                         \" --train_data_dir=\"E:\\Game\n                         2023\\AIWAIFU\\kohya_ss20230512\\kohya_ss\\kohya_ss\\YMAKELORA\\Model\\20230605\\Aya_Kujyou_Final_Gear\\\n                         Aya_Kujyou_Final_Gear\" --resolution=512,512 --output_dir=\"E:\\Game\n                         2023\\AIWAIFU\\kohya_ss20230512\\kohya_ss\\kohya_ss\\YMAKELORA\\Model\\20230605\\Aya_Kujyou_Final_Gear\\\n                         Aya_Kujyou_Final_Gear\" --network_alpha=\"128\" --save_model_as=safetensors\n                         --network_module=networks.lora --network_args conv_dim=\"1\" conv_alpha=\"1\"\n                         --text_encoder_lr=5e-05 --unet_lr=0.0001 --network_dim=16\n                         --output_name=\"Aya_Kujyou_Final_Gear-KK77-V3\" --lr_scheduler_num_cycles=\"1\"\n                         --learning_rate=\"0.0001\" --lr_scheduler=\"cosine\" --lr_warmup_steps=\"115\" --train_batch_size=\"2\"\n                         --max_train_steps=\"1150\" --save_every_n_epochs=\"1\" --mixed_precision=\"fp16\"\n                         --save_precision=\"fp16\" --seed=\"1\" --caption_extension=\".txt\" --cache_latents\n                         --optimizer_type=\"AdamW8bit\" --max_data_loader_n_workers=\"1\" --clip_skip=2\n                         --bucket_reso_steps=64 --xformers --bucket_no_upscale --wandb_api_key=\"False\"\nprepare tokenizer\nUsing DreamBooth method.\nignore directory without repeats / \u7e70\u308a\u8fd4\u3057\u56de\u6570\u306e\u306a\u3044\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u7121\u8996\u3057\u307e\u3059: sample\nprepare images.\nfound directory E:\\Game 2023\\AIWAIFU\\kohya_ss20230512\\kohya_ss\\kohya_ss\\YMAKELORA\\Model\\20230605\\Aya_Kujyou_Final_Gear\\Aya_Kujyou_Final_Gear\\50_EAV P contains 46 image files\n2300 train images with repeating.\n0 reg images.\nno regularization images / \u6b63\u5247\u5316\u753b\u50cf\u304c\u898b\u3064\u304b\u308a\u307e\u305b\u3093\u3067\u3057\u305f\n[Dataset 0]\n  batch_size: 2\n  resolution: (512, 512)\n  enable_bucket: False\n\n  [Subset 0 of Dataset 0]\n    image_dir: \"E:\\Game 2023\\AIWAIFU\\kohya_ss20230512\\kohya_ss\\kohya_ss\\YMAKELORA\\Model\\20230605\\Aya_Kujyou_Final_Gear\\Aya_Kujyou_Final_Gear\\50_EAV P\"\n    image_count: 46\n    num_repeats: 50\n    shuffle_caption: False\n    keep_tokens: 0\n    caption_dropout_rate: 0.0\n    caption_dropout_every_n_epoches: 0\n    caption_tag_dropout_rate: 0.0\n    color_aug: False\n    flip_aug: False\n    face_crop_aug_range: None\n    random_crop: False\n    token_warmup_min: 1,\n    token_warmup_step: 0,\n    is_reg: False\n    class_tokens: EAV P\n    caption_extension: .txt\n\n\n[Dataset 0]\nloading image sizes.\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 46/46 [00:00<00:00, 5749.39it/s]\nprepare dataset\npreparing accelerator\nUsing accelerator 0.15.0 or above.\nloading model for process 0/1\nload StableDiffusion checkpoint: E:\\Game 2023\\AIWAIFU\\SD15\\stable-diffusion-webui\\models\\Stable-diffusion\\yesmix_v16Original.safetensors\nloading u-net: <All keys matched successfully>\nloading vae: <All keys matched successfully>\nloading text encoder: <All keys matched successfully>\nCrossAttention.forward has been replaced to enable xformers.\nimport network module: networks.lora\n[Dataset 0]\ncaching latents.\n  0%|                                                                                           | 0/46 [00:00<?, ?it/s]\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 E:\\Game 2023\\AIWAIFU\\kohya_ss20230606_1\\kohya_ss\\train_network.py:864 in <module>                \u2502\n\u2502                                                                                                  \u2502\n\u2502   861 \u2502   args = parser.parse_args()                                                             \u2502\n\u2502   862 \u2502   args = train_util.read_config_from_file(args, parser)                                  \u2502\n\u2502   863 \u2502                                                                                          \u2502\n\u2502 \u2771 864 \u2502   train(args)                                                                            \u2502\n\u2502   865                                                                                            \u2502\n\u2502                                                                                                  \u2502\n\u2502 E:\\Game 2023\\AIWAIFU\\kohya_ss20230606_1\\kohya_ss\\train_network.py:194 in train                   \u2502\n\u2502                                                                                                  \u2502\n\u2502   191 \u2502   \u2502   vae.requires_grad_(False)                                                          \u2502\n\u2502   192 \u2502   \u2502   vae.eval()                                                                         \u2502\n\u2502   193 \u2502   \u2502   with torch.no_grad():                                                              \u2502\n\u2502 \u2771 194 \u2502   \u2502   \u2502   train_dataset_group.cache_latents(vae, args.vae_batch_size, args.cache_laten   \u2502\n\u2502   195 \u2502   \u2502   vae.to(\"cpu\")                                                                      \u2502\n\u2502   196 \u2502   \u2502   if torch.cuda.is_available():                                                      \u2502\n\u2502   197 \u2502   \u2502   \u2502   torch.cuda.empty_cache()                                                       \u2502\n\u2502                                                                                                  \u2502\n\u2502 E:\\Game 2023\\AIWAIFU\\kohya_ss20230606_1\\kohya_ss\\library\\train_util.py:1422 in cache_latents     \u2502\n\u2502                                                                                                  \u2502\n\u2502   1419 \u2502   def cache_latents(self, vae, vae_batch_size=1, cache_to_disk=False, is_main_process=  \u2502\n\u2502   1420 \u2502   \u2502   for i, dataset in enumerate(self.datasets):                                       \u2502\n\u2502   1421 \u2502   \u2502   \u2502   print(f\"[Dataset {i}]\")                                                       \u2502\n\u2502 \u2771 1422 \u2502   \u2502   \u2502   dataset.cache_latents(vae, vae_batch_size, cache_to_disk, is_main_process)    \u2502\n\u2502   1423 \u2502                                                                                         \u2502\n\u2502   1424 \u2502   def is_latent_cacheable(self) -> bool:                                                \u2502\n\u2502   1425 \u2502   \u2502   return all([dataset.is_latent_cacheable() for dataset in self.datasets])          \u2502\n\u2502                                                                                                  \u2502\n\u2502 E:\\Game 2023\\AIWAIFU\\kohya_ss20230606_1\\kohya_ss\\library\\train_util.py:814 in cache_latents      \u2502\n\u2502                                                                                                  \u2502\n\u2502    811 \u2502   \u2502   \u2502   img_tensors = torch.stack(images, dim=0)",
        ">For the majority of these 6 months I've been using regularization images. When training styles. Which people swear up and down you must not. \n\nI don't use reg images, I don't even use captions unless I struggle to get the focus. The original training papers and such don't even mention regulation. Since the method and math originates from the papers - I am confident that random people making anime titties aren't any more knowledgeable. \n\n>A lot of advice on regularization and even training is bullshit. Really, it's a sea of bullshit out there and people keep repeating stuff they heard somewhere else without looking into it as if they're proselytizing their new religion.\nI find that all community sources, especially on social media/youtube and such with flashy titles and \" _This is how you can make realistic professional amazing art images FOR FREE! Automatic1111 RunPod AnimeTits_ \" flat out don't know what they are doing. And having watched few out of curiosity I know that for sure. \n\nWhat tilts me a fair bit is that: Beyond some of the more arcane aspects, all the details and information on how these things work is on the papers, githubs or other similar sources and documentation. Obviously you can't really know whether something is actually implemented and working correctly.\n\nBy bar for whether a source is worth a damn, is really whether they cite the sources correctly. No I don't mean \"link to where I read this\" but citing and sourcing at least in manner that would pass in academic or professional setting. \n\nBut when it comes to the discussion or reg images. If you get good quality results that meet the criteria you set, without them. Do you actually need them? Because some issue people say can and should be solved with reg images, I have solved by switching the model I train from - only going to base SDXL if I am desperate. Currently the best I have come across and what I use is FluentlyXL, no idea why but it just gives me cleaner results.\n\n>You mean like the passage of time influences training results and generation? I have been training loras for over 6 months and have not experienced this at all. But these two extra factors I mentioned absolutely do influence training and generation.\n\nNot the \"passage of time\" per se. Since all the random components in the training are generated from clock, which is fiddly piece of tech. Minute alternations on what the time is affect things.  Nothing in these are 100% deterministic - we are talking about something that is inherently statistical afterall. But the time is never the reason why something works or doesn't work. It is just... tiny little things. But tiny little things can cause noticeable differences, but not to degree where it is \"did work\" or \"didn't work\".\n\nLike I talk alot about this stuff, since it is my current obsessive hobby (well training is. I am quite bad at generating, I just like the puzzle of training). And I always wonder whether I am doing \"something wrong\" to what other people are doing, because I can get things done just fine without captions, reg images, or... other bullshit reddit and whatever deems absolutely mandatory for getting good quality results. I don't do any of that and I get good results. If I can't get something to wrong I deep dive to documentation and papers, and the answers are generally just there in clear print. ",
        "fuck, it worked for me yesterday, but I only got black images due to some error with bitsandbytes. Then today I was back at Version 3.11.7 somehow?? I now got it uninstalled, but the \"AttributeError: 'NoneType' object has no attribute 'startswith'\" is back.\nI hate my life xD",
        "@hablaba watch first those videos. they have shown all settings. so this PR must be discarded.  And stop promoting AItrepeneur and compare with me . his paywalls are many times bigger than me. he got 3k paid members on Patreon \n\n if you have better tutorials than mine you can do your own pull request"
    ],
    "F11": [
        "This is an issue that should be directly to kohya's sd-script repo. I only copy his script files and this is not one I maintain.",
        "same",
        "Is it possible to start tensorboard by default without pushing this button?",
        "The ubuntu_setup.sh has an error but doesn't block the rest of the script:\nroot@ba79bdcd8d42:/workspace/kohya_ss# bash ubuntu_setup.sh\ninstalling tk\nubuntu_setup.sh: line 3: sudo: command not found\n\nIs it something expected?",
        "has this been implemented yet?",
        "IPEX 2.1.30+xpu has regressions, reverting it.",
        "One day I might research the subject and see if it would be implemented... But since the GUI is just a layer on top of the various CLI I am not sure how that would work. At 1st kohya would need to implement it in his own code. When he does I might be able to make it work by passing down the ctrl-c to his cli...",
        "tried with torch 1. same issue\n\nRuntimeError: \"slow_conv2d_cpu\" not implemented for 'Half'",
        "I'm also interested about this. ",
        "never mind, found it...",
        "Answer: I was using the original repo without gui....",
        "I have the same problem (says `returned non-zero exit status 1.`)  Any help here?\n\n`Folder 100_LoraJanaDefi1st: 110 images found\nFolder 100_LoraJanaDefi1st: 11000 steps\nTotal steps: 11000\nTrain batch size: 1\nGradient accumulation steps: 1.0\nEpoch: 1\nRegulatization factor: 1\nmax_train_steps (11000 / 1 / 1.0 * 1 * 1) = 11000\nstop_text_encoder_training = 0\nlr_warmup_steps = 1100\naccelerate launch --num_cpu_threads_per_process=2 \"train_network.py\" --enable_bucket --pretrained_model_name_or_path=\"J:/AI training/Stable diffusion/stable-diffusion-webui-directml/models/Stable-diffusion/hardblend_.safetensors\" --train_data_dir=\"J:/AI training/Stable diffusion/stable-diffusion-webui-directml/SgtSixpack Training/LoraJanaDefi1st/image1st\" --resolution=512,512 --output_dir=\"J:/AI training/Stable diffusion/stable-diffusion-webui-directml/SgtSixpack Training/LoraJanaDefi1st/model1st\" --logging_dir=\"J:/AI training/Stable diffusion/stable-diffusion-webui-directml/SgtSixpack Training/LoraJanaDefi1st/log1st\" --network_alpha=\"1\" --training_comment=\"hardblend_.safetensors training\" --save_model_as=safetensors --network_module=networks.lora --text_encoder_lr=5e-05 --unet_lr=0.0001 --network_dim=8 --output_name=\"JanaDefiNigel_HarrLora\" --lr_scheduler_num_cycles=\"1\" --learning_rate=\"0.0001\" --lr_scheduler=\"cosine\" --lr_warmup_steps=\"1100\" --train_batch_size=\"1\" --max_train_steps=\"11000\" --save_every_n_epochs=\"1\" --mixed_precision=\"fp16\" --save_precision=\"fp16\" --cache_latents --optimizer_type=\"AdamW8bit\" --max_data_loader_n_workers=\"0\" --bucket_reso_steps=64 --xformers --bucket_no_upscale\n2023-06-01 18:00:03.077447: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n2023-06-01 18:00:03.077524: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2023-06-01 18:00:07.046971: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n2023-06-01 18:00:07.047048: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\nWARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n    PyTorch 2.0.0+cu118 with CUDA 1108 (you have 2.0.1+cpu)\n    Python  3.10.11 (you have 3.10.6)\n  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n  Set XFORMERS_MORE_DETAILS=1 for more details\nJ:\\AI training\\Stable diffusion\\stable-diffusion-webui-directml\\kohya_ss\\venv\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: 'Could not find module 'J:\\AI training\\Stable diffusion\\stable-diffusion-webui-directml\\kohya_ss\\venv\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n  warn(\nprepare tokenizer\nUse DreamBooth method.\nprepare images.\nfound directory J:\\AI training\\Stable diffusion\\stable-diffusion-webui-directml\\SgtSixpack Training\\LoraJanaDefi1st\\image1st\\100_LoraJanaDefi1st contains 110 image files\n11000 train images with repeating.\n0 reg images.\nno regularization images / \u6b63\u5247\u5316\u753b\u50cf\u304c\u898b\u3064\u304b\u308a\u307e\u305b\u3093\u3067\u3057\u305f\n[Dataset 0]\n  batch_size: 1\n  resolution: (512, 512)\n  enable_bucket: True\n  min_bucket_reso: 256\n  max_bucket_reso: 1024\n  bucket_reso_steps: 64\n  bucket_no_upscale: True\n\n  [Subset 0 of Dataset 0]\n    image_dir: \"J:\\AI training\\Stable diffusion\\stable-diffusion-webui-directml\\SgtSixpack Training\\LoraJanaDefi1st\\image1st\\100_LoraJanaDefi1st\"\n    image_count: 110\n    num_repeats: 100\n    shuffle_caption: False\n    keep_tokens: 0\n    caption_dropout_rate: 0.0\n    caption_dropout_every_n_epoches: 0\n    caption_tag_dropout_rate: 0.0\n    color_aug: False\n    flip_aug: False\n    face_crop_aug_range: None\n    random_crop: False\n    token_warmup_min: 1,\n    token_warmup_step: 0,\n    is_reg: False\n    class_tokens: LoraJanaDefi1st\n    caption_extension: .caption\n\n\n[Dataset 0]\nloading image sizes.\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 110/110 [00:00<00:00, 2928.93it/s]\nmake buckets\nmin_bucket_reso and max_bucket_reso are ignored if bucket_no_upscale is set, because bucket reso is defined by image size automatically / bucket_no_upscale\u304c\u6307\u5b9a\u3055\u308c\u305f\u5834\u5408\u306f\u3001bucket\u306e\u89e3\u50cf\u5ea6\u306f\u753b\u50cf\u30b5\u30a4\u30ba\u304b\u3089\u81ea\u52d5\u8a08\u7b97\u3055\u308c\u308b\u305f\u3081\u3001min_bucket_reso\u3068max_bucket_reso\u306f\u7121\u8996\u3055\u308c\u307e\u3059\nnumber of images (including repeats) / \u5404bucket\u306e\u753b\u50cf\u679a\u6570\uff08\u7e70\u308a\u8fd4\u3057\u56de\u6570\u3092\u542b\u3080\uff09\nbucket 0: resolution (320, 512), count: 100\nbucket 1: resolution (512, 512), count: 10900\nmean ar error (without repeats): 0.0004794034090909091\nprepare accelerator\nJ:\\AI training\\Stable diffusion\\stable-diffusion-webui-directml\\kohya_ss\\venv\\lib\\site-packages\\accelerate\\accelerator.py:249: FutureWarning: `logging_dir` is deprecated and will be removed in version 0.18.0 of \ud83e\udd17 Accelerate. Use `project_dir` instead.\n  warnings.warn(\nUsing accelerator 0.15.0 or above.\nloading model for process 0/1\nload StableDiffusion checkpoint: J:/AI training/Stable diffusion/stable-diffusion-webui-directml/models/Stable-diffusion/hardblend_.safetensors\nJ:\\AI training\\Stable diffusion\\stable-diffusion-webui-directml\\kohya_ss\\venv\\lib\\site-packages\\safetensors\\torch.py:98: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  with safe_open(filename, framework=\"pt\", device=device) as f:\nloading u-net: <All keys matched successfully>\nloading vae: <All keys matched successfully>\nloading text encoder: <All keys matched successfully>\nCrossAttention.forward has been replaced to enable xformers.\n[Dataset 0]\ncaching latents.\n  0%|                                                                                          | 0/110 [00:00<?, ?it/s]\nTraceback (most recent call last):\n  File \"J:\\AI training\\Stable diffusion\\stable-diffusion-webui-directml\\kohya_ss\\train_network.py\", line 783, in <module>\n    train(args)\n  File \"J:\\AI training\\Stable diffusion\\stable-diffusion-webui-directml\\kohya_ss\\train_network.py\", line 157, in train\n    train_dataset_group.cache_latents(vae, args.vae_batch_size, args.cache_latents_to_disk, accelerator.is_main_process)  File \"J:\\AI training\\Stable diffusion\\stable-diffusion-webui-directml\\kohya_ss\\library\\train_util.py\", line 1399, in cache_latents\n    dataset.cache_latents(vae, vae_batch_size, cache_to_disk, is_main_process)\n  File \"J:\\AI training\\Stable diffusion\\stable-diffusion-webui-directml\\kohya_ss\\library\\train_util.py\", line 812, in cache_latents\n    latents = vae.encode(img_tensors).latent_dist.sample().to(\"cpu\")\n  File \"J:\\AI training\\Stable diffusion\\stable-diffusion-webui-directml\\kohya_ss\\venv\\lib\\site-packages\\diffusers\\models\\vae.py\", line 566, in encode\n    h = self.encoder(x)\n  File \"J:\\AI training\\Stable diffusion\\stable-diffusion-webui-directml\\kohya_ss\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"J:\\AI training\\Stable diffusion\\stable-diffusion-webui-directml\\kohya_ss\\venv\\lib\\site-packages\\diffusers\\models\\vae.py\", line 130, in forward\n    sample = self.conv_in(sample)\n  File \"J:\\AI training\\Stable diffusion\\stable-diffusion-webui-directml\\kohya_ss\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"J:\\AI training\\Stable diffusion\\stable-diffusion-webui-directml\\kohya_ss\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File \"J:\\AI training\\Stable diffusion\\stable-diffusion-webui-directml\\kohya_ss\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\nRuntimeError: \"slow_conv2d_cpu\" not implemented for 'Half'\nTraceback (most recent call last):\n  File \"J:\\AI training\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"J:\\AI training\\Python310\\lib\\runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"J:\\AI training\\Stable diffusion\\stable-diffusion-webui-directml\\kohya_ss\\venv\\Scripts\\accelerate.exe\\__main__.py\", line 7, in <module>\n  File \"J:\\AI training\\Stable diffusion\\stable-diffusion-webui-directml\\kohya_ss\\venv\\lib\\site-packages\\accelerate\\commands\\accelerate_cli.py\", line 45, in main\n    args.func(args)\n  File \"J:\\AI training\\Stable diffusion\\stable-diffusion-webui-directml\\kohya_ss\\venv\\lib\\site-packages\\accelerate\\commands\\launch.py\", line 923, in launch_command\n    simple_launcher(args)\n  File \"J:\\AI training\\Stable diffusion\\stable-diffusion-webui-directml\\kohya_ss\\venv\\lib\\site-packages\\accelerate\\commands\\launch.py\", line 579, in simple_launcher\n    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\nsubprocess.CalledProcessError: Command '['J:\\\\AI training\\\\Stable diffusion\\\\stable-diffusion-webui-directml\\\\kohya_ss\\\\venv\\\\Scripts\\\\python.exe', 'train_network.py', '--enable_bucket', '--pretrained_model_name_or_path=J:/AI training/Stable diffusion/stable-diffusion-webui-directml/models/Stable-diffusion/hardblend_.safetensors', '--train_data_dir=J:/AI training/Stable diffusion/stable-diffusion-webui-directml/SgtSixpack Training/LoraJanaDefi1st/image1st', '--resolution=512,512', '--output_dir=J:/AI training/Stable diffusion/stable-diffusion-webui-directml/SgtSixpack Training/LoraJanaDefi1st/model1st', '--logging_dir=J:/AI training/Stable diffusion/stable-diffusion-webui-directml/SgtSixpack Training/LoraJanaDefi1st/log1st', '--network_alpha=1', '--training_comment=hardblend_.safetensors training', '--save_model_as=safetensors', '--network_module=networks.lora', '--text_encoder_lr=5e-05', '--unet_lr=0.0001', '--network_dim=8', '--output_name=JanaDefiNigel_HarrLora', '--lr_scheduler_num_cycles=1', '--learning_rate=0.0001', '--lr_scheduler=cosine', '--lr_warmup_steps=1100', '--train_batch_size=1', '--max_train_steps=11000', '--save_every_n_epochs=1', '--mixed_precision=fp16', '--save_precision=fp16', '--cache_latents', '--optimizer_type=AdamW8bit', '--max_data_loader_n_workers=0', '--bucket_reso_steps=64', '--xformers', '--bucket_no_upscale']' returned non-zero exit status 1.`\n\n",
        "the Locon models trained is not sure whether it is able to run utilities in this gui or not, and probably not.",
        "LoCon are now supported in the master branch. Just select the LoCon LoRA type in the Dreambooth LoRA tab:\n\n![image](https://user-images.githubusercontent.com/7474674/222534724-22154e8e-19fc-42e1-a919-c1f3983a0423.png)\n\nIt will then make those 2 new parameters visible when selected:\n\n![image](https://user-images.githubusercontent.com/7474674/222534821-11e9aac5-0bc6-40ca-a6a8-65266dbb41f6.png)\n\nIf using LoCon you can try to cut the original LoRA rank in half and make the LoCon conv rank the same as the LoRA rank as a 1st start.",
        "I also encountered this problem. Is there any solution to the forced download?",
        "> turn off 8bit adam. that worked for me.\n\nhow do I turn off 8bit adam?",
        "Message from Automatic1111 log:\nFailed to match keys when loading Lora C:\\SD\\stable-diffusion-webui\\models\\Lora\\FT40.21-step00000400.safetensors",
        "Do not feel like troubleshooting the conflict ;-) I will manually bump the gradio version in the requirements file.",
        "I merged it to master. Should be good to go.",
        "@bmaltais i have allready tried to port back dependencies (for example i have used 100% identical requirements in 23.x that work in 21/22.3) with no luck. also i did run acceleraor config manually aswell.\ni really think this is an issue in the scripts itself\n",
        "Any update on this? I am still having this issue on MacOS (M1)",
        "Ok so apparently everything is working properly based on diagnostics . This message is just letting the user know, that stuff will be configured to use xformers. I thought it was telling me to replace or update something. ",
        "Ahh. I didn't realize until I did some looking that `train_network.py` is technically part of Kohya's code ",
        "run pip install --r requirements.txt\nin venv\n",
        "Same problem here, https://github.com/bmaltais/kohya_ss/issues/1163\n\nDropdown the version to 21.7.16."
    ]
}